{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MP5rRkbTpnG8"
      },
      "source": [
        "# **[Diff-SVC](https://github.com/prophesier/diff-svc)**\n",
        "Singing Voice Conversion via diffusion model\n",
        "\n",
        "____\n",
        "\n",
        "####  **Notebook put together by [justinjohn-03](https://github.com/justinjohn0306)**\n",
        "#### **Edited for batch rendering by [MLo7](https://github.com/MLo7Ghinsan)**\n",
        "\n",
        "## **Special thanks to [prophesier](https://github.com/prophesier) and [UtaUtaUtau](https://github.com/UtaUtaUtau)**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Unmount your Gdrive (If you need)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.flush_and_unmount()\n",
        "!rm -rf /content/drive"
      ],
      "metadata": {
        "id": "e2ckFYIOh43e",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8nnusihSv2a9",
        "outputId": "9e84217c-f4e4-4039-eecc-b0cc30ab4528"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title ## 1. Mount your Gdrive \n",
        "#@markdown (This is an essential step if you want to load your own trained model)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mux_wwBggJKB",
        "outputId": "24c666d2-0174-4420-c3f4-b6fc256d3ff0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!\n"
          ]
        }
      ],
      "source": [
        "#@title # 2. Setup - Install Diff-SVC\n",
        "#@markdown \"Repository : UtaUtaUtau\" makes better results\n",
        "from IPython.display import clear_output \n",
        "from google.colab import files \n",
        "import os\n",
        "\n",
        "!rm -rf /content/sample_data\n",
        "\n",
        "\n",
        "Mode = \"install\" #@param [\"install\", \"update\", \"remove\"]\n",
        "Repository = \"UtaUtaUtau\" #@param [\"Official Diff-SVC\", \"UtaUtaUtau\"]\n",
        "Branch_name = \"\" #@param {type:\"string\"}\n",
        "\n",
        "repositories = {\n",
        "  'Official Diff-SVC':'prophesier',\n",
        "  'UtaUtaUtau':'UtaUtaUtau'\n",
        "}\n",
        "\n",
        "from pathlib import Path\n",
        "if Mode == 'install':\n",
        "  git_cmd = ''\n",
        "  if Branch_name: git_cmd += f\"-b {Branch_name} \"\n",
        "\n",
        "  git_cmd += f\"--depth 1 https://github.com/{repositories[Repository]}/diff-svc.git\"\n",
        "  !git clone $git_cmd\n",
        "  %cd /content/diff-svc\n",
        "  print('Installing torch')\n",
        "  !pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html\n",
        "\n",
        "  !pip install tensorboard<2.9,>=2.8\n",
        "  %reload_ext tensorboard\n",
        "  print('Downloading pretrained models')\n",
        "  %cd \"/content/\"\n",
        "  %mkdir -p /content/diff-svc/checkpoints/\n",
        "  !wget https://github.com/MLo7Ghinsan/MLo7_Diff-SVC_models/releases/download/diff-svc-necessary-checkpoints/0102_xiaoma_pe.zip\n",
        "  !wget https://github.com/MLo7Ghinsan/MLo7_Diff-SVC_models/releases/download/diff-svc-necessary-checkpoints/0109_hifigan_bigpopcs_hop128.zip\n",
        "  !wget https://github.com/MLo7Ghinsan/MLo7_Diff-SVC_models/releases/download/diff-svc-necessary-checkpoints/nsf_hifigan.zip\n",
        "  !wget https://github.com/MLo7Ghinsan/MLo7_Diff-SVC_models/releases/download/diff-svc-necessary-checkpoints/hubert.zip\n",
        "  !mkdir /content/diff-svc/checkpoints/0102_xiaoma_pe\n",
        "  !mkdir /content/diff-svc/checkpoints/0109_hifigan_bigpopcs_hop128\n",
        "  !mkdir /content/diff-svc/checkpoints/nsf_hifigan\n",
        "  !mkdir /content/diff-svc/checkpoints/hubert\n",
        "  !unzip /content/0102_xiaoma_pe.zip -d /content/diff-svc/checkpoints/0102_xiaoma_pe\n",
        "  !unzip /content/0109_hifigan_bigpopcs_hop128.zip -d /content/diff-svc/checkpoints/0109_hifigan_bigpopcs_hop128\n",
        "  !unzip /content/nsf_hifigan.zip -d /content/diff-svc/checkpoints/nsf_hifigan\n",
        "  !unzip /content/hubert.zip -d /content/diff-svc/checkpoints/hubert\n",
        "\n",
        "  !rm /content/0102_xiaoma_pe.zip\n",
        "  !rm /content/0109_hifigan_bigpopcs_hop128.zip\n",
        "  !rm /content/nsf_hifigan.zip\n",
        "  !rm /content/hubert.zip\n",
        "\n",
        "  !rm /content/diff-svc/results/test_output.wav\n",
        "\n",
        "\n",
        "  clear_output()\n",
        "\n",
        "  print('Done!')\n",
        "\n",
        "elif Mode == 'update':\n",
        "  %cd /content/diff-svc\n",
        "  !git pull\n",
        "  !pip install -r requirements_short.txt\n",
        "  clear_output()\n",
        "  print(\"Done!\")\n",
        "else:\n",
        "  answer = input(\"Are you sure you want to delete diff-svc folder? (y/n)\").lower()\n",
        "  while answer not in [\"y\", \"n\"]:\n",
        "    print(\"Invalid input\")\n",
        "    answer = input(\"Are you sure you want to delete diff-svc folder? (y/n)\").lower()\n",
        "  if answer == \"y\":\n",
        "    %cd /content\n",
        "    %rm -r diff-svc/\n",
        "    print(\"Done!\")\n",
        "  else:\n",
        "    print(\"Cancelled...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZB_vtSNaNgz"
      },
      "outputs": [],
      "source": [
        "!pip install torchcrepe==0.0.17;\n",
        "!pip install praat-parselmouth==0.4.1;\n",
        "!pip install scikit-image==0.19.3;\n",
        "!pip install ipython==8.5.0;\n",
        "!pip install ipykernel==6.16.2;\n",
        "!pip install pyloudnorm==0.1.0;\n",
        "!pip install webrtcvad==2.0.10;\n",
        "!pip install h5py==3.7.0;\n",
        "!pip install einops==0.5.0;\n",
        "!pip install pycwt==0.3.0a22;\n",
        "!pip install torchmetrics==0.5;\n",
        "!pip install pytorch_lightning==1.3.3;\n",
        "!pip install pyworld==0.3.1;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7rtUN0uPu9h",
        "outputId": "8166019c-307f-4e72-c7e3-39a95b4f2691"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GNN-for-text-classification'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 36 (delta 3), reused 31 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (36/36), 1.52 MiB | 6.78 MiB/s, done.\n",
            "/content/GNN-for-text-classification\n",
            "config.py  \u001b[0m\u001b[01;34mdata\u001b[0m/  main.py  \u001b[01;34mmodels\u001b[0m/  \u001b[01;34mpreprocess\u001b[0m/  README.md  \u001b[01;34mresult\u001b[0m/  \u001b[01;34mutils\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/zshicode/GNN-for-text-classification.git\n",
        "%cd GNN-for-text-classification/\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awzgZDSuPyZK",
        "outputId": "53f5d6c5-3c73-4233-e1ed-6bd9158d62da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2022.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.22.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZHaH4K7RCFV",
        "outputId": "90e4c540-aa13-4b7d-87f9-9db3bd7caaba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hparams\n",
            "  Downloading hparams-0.3.0-py3-none-any.whl (11 kB)\n",
            "Collecting typeguard (from hparams)\n",
            "  Downloading typeguard-3.0.2-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from typeguard->hparams) (4.5.0)\n",
            "Installing collected packages: typeguard, hparams\n",
            "Successfully installed hparams-0.3.0 typeguard-3.0.2\n"
          ]
        }
      ],
      "source": [
        "! pip install hparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dlM9RgYviRhy",
        "outputId": "e05cecb1-46e1-4ecd-e179-26383ebf6a62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/diff-svc\n"
          ]
        }
      ],
      "source": [
        "#@title # **3. Load model**\n",
        "\n",
        "#@markdown ### **Load the pretrained model (default)**\n",
        "\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown Note: Add the full path to the most recent checkpoint located on your Gdrive as well as the speaker's name if you wish to use your own model.\n",
        "\n",
        "#@markdown Example:-\n",
        "\n",
        "#@markdown The ``project_name`` will be the name of your speaker\n",
        "\n",
        "#@markdown  ``model_path: /content/drive/MyDrive/Diff-SVC/checkpoints/model_name/model_ckpt_steps_50000.ckpt``\n",
        "\n",
        "#@markdown           ``config_path: /content/drive/MyDrive/Diff-SVC/checkpoints/model_name/config.yaml``\n",
        "\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown ### **Set model location with the name of the speaker:**\n",
        "#@markdown *If you wish to use the pre-trained model and don't have your own model, leave these at their default values.*\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "%cd \"/content/diff-svc/\"\n",
        "\n",
        "os.environ['PYTHONPATH']='.'\n",
        "\n",
        "!CUDA_VISIBLE_DEVICES=0\n",
        "\n",
        "\n",
        "from utils.hparams import hparams\n",
        "from preprocessing.data_gen_utils import get_pitch_parselmouth,get_pitch_crepe\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython.display as ipd\n",
        "import utils\n",
        "import librosa\n",
        "import torchcrepe\n",
        "from infer import *\n",
        "import logging\n",
        "from infer_tools.infer_tool import *\n",
        "\n",
        "logging.getLogger('numba').setLevel(logging.WARNING)\n",
        "\n",
        "# 工程文件夹名，训练时用的那个\n",
        "project_name = \"chim\" #@param {type: \"string\"}\n",
        "model_path = \"/content/drive/MyDrive/singingminsu_2/chim_model/model_ckpt_steps_14000.ckpt\" #@param {type: \"string\"}\n",
        "config_path=\"/content/drive/MyDrive/singingminsu_2/chim_model/config.yaml\" #@param {type: \"string\"}\n",
        "hubert_gpu=True\n",
        "svc_model = Svc(project_name,config_path,hubert_gpu, model_path)\n",
        "print('model loaded')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n34McW_Q0-iF"
      },
      "outputs": [],
      "source": [
        "#@title # 4. Import your audio\n",
        "\n",
        "%cd \"/content/diff-svc\"\n",
        "\n",
        "#@markdown File location in Drive\n",
        "\n",
        "#@markdown Or you can simply drag and drop your files into batch_audio folder after running this cell (remove the directory in this cell first)\n",
        "audio_location = '/content/drive/MyDrive/singingminsu_2/data/afterlike_wave.wav' #@param {type: \"string\"}\n",
        "!mkdir -p batch_audio\n",
        "audio_clone = \"batch_audio\"\n",
        "!rm /raw/test_input.wav\n",
        "if audio_location.endswith('.rar'):\n",
        "    !unrar x \"$audio_location\" \"$audio_clone\"\n",
        "elif audio_location.endswith('.zip'):\n",
        "    !unzip \"$audio_location\" -d \"$audio_clone\"\n",
        "elif audio_location.endswith('.tar'):\n",
        "    !tar -xf \"$audio_location\" -C \"$audio_clone\"\n",
        "elif audio_location.endswith('.tar.gz'):\n",
        "    !tar -xzf \"$audio_location\" -C \"$audio_clone\"\n",
        "elif audio_location.endswith('.tar.bz2'):\n",
        "    !tar -xjf \"$audio_location\" -C \"$audio_clone\"\n",
        "else:\n",
        "    !7za x \"$audio_location\" -o$audio_clone\n",
        "  \n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DauP3lXimfS3",
        "outputId": "9b76486f-f701-4108-b73c-ea47194ee4bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/diff-svc/batch_audio/afterlike_wave.wav\n",
            "code version:2022-12-04\n",
            "executing 'slice' costed 2.550s\n",
            "#=====segment start, 2.442s======\n",
            "jump empty segment\n",
            "#=====segment start, 21.335s======\n",
            "executing 'get_pitch' costed 11.169s\n",
            "hubert (on cuda) time used 5.319851875305176\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sample time step: 100%|██████████| 100/100 [00:03<00:00, 30.29it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "executing 'diff_infer' costed 3.355s\n",
            "executing 'after_infer' costed 0.976s\n"
          ]
        }
      ],
      "source": [
        "#@title # 5. Input audio and adjust parameters\n",
        "\n",
        "#@markdown ### <b><font color=\"red\"> Make sure that your imported audio have no subfolders before running this cell\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown #### ``This shifts the raw audio up by one semitone before rendering, if the raw input is of a male voice and the desired voice is female, you can input 8 or 12 etc (12 would shift a whole octave).``\n",
        "key = 0#@param {type: \"integer\"}\n",
        "# 加速倍数\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown #### ``The multiple of the inference acceleration , the default value is 1000 steps, inputting a value if 10 would mean only using 100 steps to render, it's a rather straightforward value. The value can go up to 50x (rendering in 20 steps) without causing audible quality loss, if the value is set any higher it may start to cause quality loss.``\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "\n",
        "#@markdown #### Note: ``If use_gt_mel is set to True below, you should keep this value lower than the add_noise_step value and keep it at a value where it can completely divide 1000.``\n",
        "\n",
        "\n",
        "pndm_speedup = 10 #@param {type: \"integer\"}\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown #### ``Related to the use_gt_mel parameter, it controls the balance of the input and target voice, a value of 1 is completely the raw input, a value of 1000 is completely the target voice, there's an audible mix in tone when the value falls around 300 (this value isn't linear, also, if this parameter is set very low, you can decrease the pndm exceleration value for higher rendering quality)``\n",
        "add_noise_step = 1000 #@param {type: \"integer\"}\n",
        "#@markdown ___\n",
        "#@markdown #### ``Crepe's noise filter threshold, you can increase the value of the raw audio is clean, and if there is a lot of noise, you can keep or decrease the value, changing the use_crepe parameter to False will disable this parameter.``\n",
        "thre = 1 #@param {type: \"integer\"}\n",
        "#@markdown ___\n",
        "#@markdown #### ``Crepe is a F0 calculation algorithm, it's good but slow, setting the value to False will change the F0 calculation algorithm from crepe to parselmouth that is faster than crepe but is of lower quality``\n",
        "use_crepe= False #@param {type:\"boolean\"}\n",
        "#@markdown ___\n",
        "#@markdown #### ``F0 extraction algorithm for MEL spectogram rendering, using False will use the raw input's F0 for rendering. There's usually a difference in output between using True and False for rendering, usually setting it to True yields better results, but it's not set in stone, either value doesn't impact rendering speeds much. (Whatever the key value is, this is always changeable, doesn't affect it)``\n",
        "use_pe=True #@param {type:\"boolean\"}\n",
        "#@markdown ___\n",
        "#@markdown #### ``This option is similar to the image-to-image function of AI art generation, if set to True, the output audio shall be a mix of the input voice and the target voice, the percentage of each is decided by the next parameter.``\n",
        "\n",
        "#@markdown #### ``NOTE!!!: If this parameter is set to true, keep the key parameter value at 0, as rendering with various pitch input is not supported.``\n",
        "use_gt_mel= False #@param {type:\"boolean\"}\n",
        "#@markdown ___\n",
        "#@markdown #### ``The folder of your audio files, default is batch_audio``\n",
        "folder = \"/content/diff-svc/batch_audio\" #@param {type: \"string\"}\n",
        "#@markdown ___\n",
        "for name in os.listdir(folder): \n",
        "    if name.endswith(\".wav\"):\n",
        "        wav_fn = os.path.join(folder, name)\n",
        "        print(wav_fn)\n",
        "        out_folder = \"results\"\n",
        "        wav_gen = os.path.join(out_folder, name)\n",
        "        f0_tst, f0_pred, audio = run_clip(svc_model,file_path=wav_fn, key=key, acc=pndm_speedup, use_crepe=use_crepe, use_pe=use_pe, thre=thre,\n",
        "                                        use_gt_mel=use_gt_mel, add_noise_step=add_noise_step,project_name=project_name,out_path=wav_gen)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoungfUq8vA5",
        "outputId": "8555eaca-af9e-4b4d-faab-8f30876a9f1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: results/ (stored 0%)\n",
            "  adding: results/afterlike_wave.wav (deflated 21%)\n"
          ]
        }
      ],
      "source": [
        "#@title # 6. Zip up the result to your drive\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "zip_exp_name = \"chimfter\" #@param {type:\"string\"}\n",
        "\n",
        "!mkdir /content/drive/MyDrive/Diff-SVC-RESULTS\n",
        "!zip -r \"/content/drive/MyDrive/Diff-SVC-RESULTS/{zip_exp_name}.zip\" \"results\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "8KCEfSI52R_m"
      },
      "outputs": [],
      "source": [
        "#@markdown # Delete old inputted wav and rendered wav\n",
        "\n",
        "#@markdown ___\n",
        "\n",
        "#@markdown Run this cell if you want to redo the process, this cell will flush every .wav in results folder and batch_audio folder\n",
        "!rm -rf /content/diff-svc/batch_audio/*.wav\n",
        "!rm -rf /content/diff-svc/results/*.wav"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}